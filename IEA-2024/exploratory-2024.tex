\input{ERCstyle}
%\chead{\fancyplain{}{Part B1}}
\usepackage{xspace}

\begin{document}
%\begin{titlepage}
%\thispagestyle{empty}
%\begin{center}
%\textbf{ERC Starting Grant 2023}\\
%\textbf{Research proposal [Part B1]}\\
%\vspace*{2cm}
%\begin{LARGE}
%\textbf{\prnamelong{}}\\
%\textbf{\prname{}}\\
%\end{LARGE}
%\vspace*{2cm}
%\begin{Large}
%\begin{tabular}{rl}
%Principal investigator: & \piname{}\\
%Host institution:&  \hi{}\\
%Duration:& 60 months
%\end{tabular}
%\end{Large}
%\date{}
%\end{center}
%\vspace*{2cm}
%\input{ERCsummary}
%\end{titlepage}
%\clearpage
%\newpage

\renewcommand{\prname}{LOTCON\xspace}

\begin{Large}
\begin{tabular}{rl}
Project: & {\sc LOw-rank Tensor representations of}\\
& {\sc CONvolution neural networks (LOTCON)}\\
Principal investigator: & Suraj KUMAR\\
Host institution:&  LIP, ENS \& Inria Lyon
\end{tabular}
\end{Large}

\begin{abstract}
	TODO...
\end{abstract}
%\section{Evaluation Criteria}
%\subsection{Research Project}
%Ground-breaking nature, ambition and feasibility
%
%\paragraph{Ground-breaking nature and potential impact of the research project}
%\begin{enumerate}
%	\item To what extent does the proposed research address important challenges?
%	\item To what extent are the objectives ambitious and beyond the state of the art (e.g. novel concepts and
%	approaches or development between or across disciplines)?
%	\item To what extent is the proposed research high risk-high gain (i.e. if successful the payoffs will be very
%	significant, but there is a high risk that the research project does not entirely fulfill its aims)?
%\end{enumerate}
%\paragraph{Scientific Approach}
%\begin{enumerate}
%	\item To what extent is the outlined scientific approach feasible bearing in mind the extent that the
%	proposed research is high risk/high gain (based on the Extended Synopsis)?
%	\item To what extent are the proposed research methodology and working arrangements appropriate to
%	achieve the goals of the project (based on the full Scientific Proposal)?
%	\item To what extent does the proposal involve the development of novel methodology (based on the full
%	Scientific Proposal)?
%	\item To what extent are the proposed timescales, resources and PI commitment adequate and properly
%	justified (based on the full Scientific Proposal)?
%\end{enumerate}
%\subsection{Principal Investigator}
%Intellectual capacity and creativity
%
%\begin{enumerate}
%	\item To what extent has the PI demonstrated the ability to conduct ground-breaking research?
%	\item To what extend does the PI provide evidence of creative independent thinking?
%	\item To what extent does the PI have the required scientific expertise and capacity to successfully execute
%	the project?
%\end{enumerate}
%
%Keywords: Feasibility, Independence, multi-disciplinary 
%Hiring: 4 PhD students + 2 Postdocs
%\section*{B1.a. Extended synopsis}
%
%\textit{The extended synopsis should have a maximum of 5 pages, not including references.}\\
%
%The Extended Synopsis should give a concise presentation of the scientific proposal, with particular attention to the ground-breaking nature of the research project, which will allow evaluation panels to assess, in Step 1 of the evaluation, the feasibility of the outlined scientific approach. Describe the proposed work in the context of the state of the art of the field. References to literature should also be included. Please use a reference style that is commonly used in your discipline such as American Chemical Society (ACS) style, American Medical Association (AMA) style, Modern Language Association (MLA) style, etc. and that allows the evaluators to easily retrieve each reference.
%%\input{B1A}


%Deep neural networks  (DNNs) are currently the state-of-the-art models to classify objects in several domains, such as computer vision, speech recognition, text processing etc. Thanks to improved computational capability, we witnessed several popular complex and deeper DNNs. For example, AlexNet is 8 layers deep, while ResNet employs short connections and is represented with 152 layers. Both have about 60M parameters. ResNet is about a 10\% better in terms of accuracy. As  ResNet has more number of layers, therefore its computational requirements are (about $10\times$) more than AlexNet. Generative Pre-trained Transformer (GPT) models are now the state-of-the-art in Natural Language Processing, being deep and wide at the same time, having large number of parameters. For  example, GPT-4 has about 175B parameters. Its successor, GPT-4, has 100T parameters. Working (Training and prediction) with such models requires significant amount of computing resources. It is therefore necessary to devise parallel approaches which can exploit the full capabilities of the modern computing systems.    





%Recent contributions have been made to replace dense weight matrices of the fully connected layers of a deep neural network by tensor representations. This approach drastically reduces the number of parameters of the network while achieving the similar accuracy. This representation also enables one to apply more ideas and algorithms from physics and chemistry to train the network efficiently. For example, the network trained using the density matrix renormalization group (DMRG, a popular algorithm in the molecular simulation community) achieves superior accuracy compared to the conventional methods. Tensors based neural networks are much simpler than the conventional networks. Hence interpretation of these networks is relatively less complex and allows one to obtain more insight of the networks.

%TODO: CNN work by Oseledets
\subsection*{Motivation}
%\subsection*{State of the art}
%\textbf{Recommended length: 1.5 pages}\\

%\textit{What is the problem that is addressed in this project? Describe the state of the art and explain why the current state of the art is not sufficient to solve the problem addressed in the project.} 


%\begin{itemize}
%	\item Details of CNN network
%	\item Current work on replacing some parts of CNN with popular low-rank tensor representations for training
%	\item Tucker tensor decomposition and Tensor train decomposition
%	\item Growing popularity of low-rank tensor representation and their applications in neural networks
%\end{itemize}


Convolution neural networks  (CNNs) are currently the state-of-the-art models to classify objects in several domains, such as computer vision, speech recognition, text processing etc. Thanks to improved computational capability, we witnessed several popular complex and deeper DNNs. For example, AlexNet is 8 layers deep, while ResNet employs short connections and is represented with 152 layers. Both have about 60M parameters. ResNet is about a 10\% better in terms of accuracy. Both CNNs have very high computational requirements. Generative Pre-trained Transformer (GPT) models are now the state-of-the-art in Natural Language Processing, being deep and wide at the same time, having large number of parameters. For  example, GPT-3 has about 175B parameters. Its successor, GPT-4, has 100T parameters. Working (training and inference) with such models requires significant amount of computing resources. It is therefore necessary to devise parallel approaches which can exploit the full capabilities of the modern computing systems.    


GPUs deliver increased processing capabilities and superior energy efficiency compared to CPUs. Therefore, they have become a crucial element of many computing systems over the past decade. The traditional computing cores of GPUs provide the accuracy and precision for the mathematical operations. These cores take huge amount of time for deep learning models as they require to process massive datasets. Thus, GPU vendors recently introduced Tensor (or Matrix) cores. These cores can perform multiple operations per cycle at the cost of limited precision, whereas traditional cores perform one operation per cycle with very accurate results. For deep learning models, the newly added Tensor/Matrix are much effective in terms of both cost and computation speed. Hence these are the preferred choice for these models. However such approaches do not make full utilization of other resources. Thus, it is paramount to devise approaches that can exploit the maximum potential of the computing systems for CNNs.



Tensors are a natural way to represent high dimensional data for numerous applications in computational science and data science. Tensor decompositions help to identify inherent structure of data, achieve data compression and enable various ways of data analysis~\cite{KB-SIAMReview2009}. CP, Tucker and Tensor Train are the widely used tensor decomposition methods in the literature. These decompositions represent a high dimensional object with a small set of low dimensional objects. These decompositions can be viewed as high order generalization of Singular Value Decomposition (SVD). CP decomposition is used to understand the latent components of the data and well suited for tensors with small dimensions. Tucker decomposition is considered to be more appropriate for compression and multi-modal data analysis of small and moderate dimensional tensors. Tensor train decomposition captures the entanglement among dimensions and appropriate to work with high dimensional tensors.


Representing a high dimensional tensor with a set of small dimensional objects drastically reduces the overall number of parameters. This led to the use of low-rank tensor representations at different layers of CNNs. For example, it has been shown that replacing convolution kernels of ResNet with their low-rank approximations in Tucker representations significantly reduces the number of parameters and improves the overall performance~\cite{PSSEG+-ECCV2020}. In a separate recent work, contributions have been made to replace dense weight matrices of the fully connected layers of AlexNet by their approximations in Tensor-Train format~\cite{NPOV-NIPS2015}. This approach also significantly reduces the number of parameters while achieving the similar accuracy. Replacing a layer in CNN with its low-rank approximation requires to re-tune the network parameters and both the mentioned work adapted the existing training methods for tensor representations. The above work strongly advocates to employ the low-rank tensor representations in CNNs. We view the full CNN as a big tensor and aim to replace it with a set of smaller tensors. However the training methods for this big tensor and the decomposition structure is not intuitive to estimate. Therefore, we plan to follow bottom-up approach and represent multiple layers of existing successful CNN architectures -- AlexNet and ResNet, with their low-rank tensor representations.






%\subsection*{Target: Aug 12}
%\begin{itemize}
%	\item Popularity of neural networks (Resnet)
%	\item Number of neurons in the network
%	\item Optional: A bit may be about ChatGPT
%	\item Processing in Parallel
%	\item GPU is an essential component (Neural network centeric)
%	\item Some recent work: Inspiration from quantum inspired networks, parameter sizes, CNN (Convolution neural networks)
%	
%\end{itemize}


\subsection*{Objectives}
%\textbf{Recommended length: 1 page}\\
%
%\textit{Clearly state the objectives beyond the state of the art and the challenges that these objectives will raise.}

%\subsection*{Target}
%\begin{itemize}
%	\item Understanding and quicker training/inference of the networks
%\end{itemize}
%\subsection*{Selling points}
%\begin{itemize}
%	\item Simplicity and analysis capability
%	\item Reduction of parameters
%	\item Parallel algorithms for training and prediction
%\end{itemize}
%Inference is about understanding the relationship between the response and the predictor variables.

The state-of-the-art neural network models are very complex to understand. The high level objective of this project is to express neural network models with simpler tensor based networks. 
%This approach will also improve training and prediction time for the networks. 
The \prname project aims to drastically improve the following features of the neural network models.
\begin{itemize}
	\item \emph{Simplicity and analysis capability} : The neural networks represented with different tensor factorizations are simpler. Therefore it will allow one to gain more insights of the tensorized neural networks and describe what happens in each layer. We though do not explicitly focus on this aspect in \prname project and it is a part of our future work.
	\item \emph{Reduction of parameters} : Some researchers have replaced the certain portions of the neural networks with smaller tensor networks and achieved  similar accuracies with significantly less number of parameters, as mentioned earlier. The \prname project explores this direction with the aim to represent the full CNN by a network of smaller tensors. 
	\item \emph{Parallel algorithms for training and prediction} : Representing the neural networks with tensor based networks will allow one to take advantage of the existing tensor algorithms. Now a days parallelization is ubiquitous in most computing systems. State-of-the-art neural networks usually rely on the efficient parallel implementation of matrix multiplication for parallelization. This approach processes only one layer at a time in backward or forward propagation. Tensor based neural networks enable one to apply different parallelization schemes from Physics and Chemistry for training and inference.
\end{itemize}


\subsection*{Methodology} 
%\textbf{Recommended length: 1.5 pages}\\

%\textit{Provide an overview of the methodology that is planned to be developed. You may split the methodology into work-packages, clearly identify which are the objectives/challenges addressed by each work-package.}

As mentioned earlier, this project aims to represent the complete CNN by a network of smaller tensors and we plan to achieve in a bottom-up fashion. In the beginning, we plan to focus on AlexNet architecture. It has 5 convolution layers and 3 fully connected layers. We first plan to replace convolution layers with a network of smaller tensors and then replace the fully connected layers with another network of smaller tensors. This will require us to re-tune the parameters of the full network. To do this, we plan to adapt Gradient-descent method for training. We also plan to take advantage of the existing training methods for tensor network based models in physic or chemistry. For example, density matrix renormalization group (DMRG) is a popular algorithm in molecular simulation community and it has demonstrated initial success to train neural networks~\cite{SS-NIPS2016}. Once we have a working architecture with low rank representations of two large tensors, then our next step would be to replace both tensors with a single one. Here first we plan to represent the large tensor in one of the popular low-rank tensor representations. Depends on the challenges faced at this level, we also plan to consider combination of more than one low-rank tensor representation.    


We first will work with  MNSIT dataset for our training. After that we will also work with CIFAR and Imagenet datasets. After exploring AlexNet architecture, we will consider ResNet architecture in the next step. In general, ResNet is more complex and consists of 152 convolution layers. Here we plan to replace 4-5 layers at once with a smaller tensor network and then re-tune the parameters. Based on the outcome of this step, we plan to iterate this approach until we represent the full network with a low-rank representation of a large tensor.  

Once we present the whole network with a small tensor network. We aim to take advantage of the parallel tensor theory and apply in our framework to improve the training time. At this end, we also plan to implement parallel version of the proposed algorithms (or use them if they already exist). We also aim to offload some of the compute intensive operations on GPUs.   

%\begin{itemize}
%	\item Combine both approaches one after another with a help of an intermediate layer
%	\item Replace both structure with a small tensor network
%	\item Take advantage of existing work on parallelization of tensor operation and apply in our framework
%	\item Implementation of the proposed algorithm
%	\item Offload some operations on GPUs
%\end{itemize}

%Low level details:
%\begin{itemize}
%	\item Datasets: Start with MNSIT, and then think about CIFAR and Imagenet
%	\item Architecture: Alexnet (5+3 layers) and Resnet (152 layers)
%	\item Methodology: Start with Alexnet, replace convolution layers and fully connected layers
%\end{itemize}

\subsection*{Risk and impact}
%\textbf{Recommended length: 0.7 pages}\\

%\textit{Describe the scientific risks associated with your ambitious objectives and means to mitigate the risks (your past and recent expertise, your capacity to solve difficult problems, etc. Try to answer the following questions: if successful, what would be the impact of the project? what we would have at the end of the project that we don't have today, and why is it necessary to reach the project objectives, from a scientific point of view? Which will be the benefits for the society?}
%
%	\begin{itemize}
%	\item Need new training methods
%	\item Challenges with the parallelization of the proposed approaches
%	\item Close dependences on tasks
%\end{itemize}


There are two main risks associated with this project, but we believe they are not critical to its success. Here we also mention how do we plan to mitigate the impact of these risks.

\begin{itemize}
	\item \emph{Design of new training methods} : As mentioned in the methodology section that we plan to modify the structure of CNNs with low-rank tensor representations and this approach requires to re-tune the parameters. We plan to design new training methods by adapting the present popular algorithms for CNN sor tensor based networks. we also need to validate the robustness of the proposed methods. To achieve this, in the beginning of the project, we plan to work with an intern and analyze what are the possible ways to adapt the popular training methods for tensor-based networks. The goal of this step is to decide certain methods and try to obtain theoretical or probabilistic guarantees on the convergence of those methods.
	
	\item \emph{Parallelization of the proposed training methods} : The proposed training methods may have limited parallelism. For example, it is well known that the DMRG method that is used in molecular simulation to work with tensor networks is hard to parallelize. To overcome this limitation, we plan to consider/develop methods that have enough work at each step or can be parallelized in a tree structure. We also plan to design our methods in a modular structure. This will allow us to offload compute intensive components to GPUs.        
\end{itemize}


\subsection*{Resources}
%\textbf{Recommended length: 0.3 pages}\\
%
%\textit{Briefly describe the resources to be committed: human resources allocated to the project (including the commitment of the PI), laboratory equipment (available and to be purchased), external collaborators, etc.}
%
%\begin{itemize}
%	\item One PhD student (36 months)
%	\item One engineer (18 months)
%\end{itemize}


The major recruitments of this project are a PhD student and an engineer. The PhD student is expected to join the project from the beginning and  will focus on replacing the full CNNs  with low-rank based tensor representations and design new robust training methods. The engineer is expected to be hired for 18 month, starting around 24th month of the project. He/She will extend the proposed algorithms for parallel machine and efficiently implement them. The project also includes one master-level research intern. He/She is expected to join the project from the beginning and work on devising new training methods for tensor based CNNs and proving guarantees for them.

We plan to submit another proposal whose aim would be to get more insight of the network, for example what is the role of each component in the full network. We are also interested to extend our framework for recurrent neural network models in future. 

%Extension of this work:
%\begin{itemize}
%	\item Processing RNN models
%	\item Meaning of the proposed models
%\end{itemize}

\bibliographystyle{abbrv}
\bibliography{explore}
\end{document}

