<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Resource-aware computations on CPUs and GPUs</title>
  <link rel="stylesheet" href="anne.css" />
</head>
<body bgcolor="#ffffff" text="#000000">

<center><h1 class="title">Resource-aware computations on CPUs and GPUs</h1></center>
<center><h3>Instructors: <a href="https://,,">TBD</a>
  (<a href="http://www.ens-lyon.fr/LIP/ROMA">ROMA</a> team, LIP, ENS Lyon), 2025-2026 </h3></center><br><br>

<!--
<h1>Data-aware parallel algorithms for matrix and tensor computations</h1>

<h4>Instructor: <a href="https://surakuma.github.io/">Suraj Kumar</a>, Fall 2023 </h4>

<i>Course notes and slides:</i>
-->


<p>

Computing platforms have limited resources, such as cache, bandwidth, and processing power. Therefore, it is important to utilize these resources efficiently, which also helps to reduce energy consumption. GPUs deliver increased processing capabilities and superior energy efficiency compared to CPUs. Therefore, they have become a crucial element of many computing systems over the past decade. The goal of this course is to present approaches to utilize resources efficiently for linear algebra (matrix) computations. Initially, we will focus on computations on CPUs. Later, we will learn to efficiently exploit GPUs.
</p>


<h2>Outline of the course</h2>

<ul>
  <li> Introduction <ul>
    <li> Course details and evaluation methods</li>
    <li> Relevance of this course</li>
  </ul></li>
  <li> Computations on CPUs <ul>
      <li> Matrix multiplications</li>
      <li> I/O complexity bounds </li>
      <li> Data transfer cost models </li>
      <li> Communication avoiding sequential and parallel algorithms </li>
      <li> 2D/2.5D/3D algorithms
      <li> Directed acyclic graph (DAG) scheduling
      <li> Memory aware computations </li>
      <li> Exploratory research topics </li>
    </ul></li>

  <li> Computations on GPUs <ul>
    <li> GPU vs CPU architecture </li>
    <li> CUDA basics </li>
    <li> Memory model, threads, blocks </li>
    <li> Memory optimizations using shared memory </li>
    <li> Coalescing and memory alignment </li>
    <li> Tensor cores </li>
    <li> Exploratory research topics </li>
  </ul></li>
  We will have some programming sessions in this part.


<br>
 <p>We will also look at several interesting research projects in the course
 related to parallel computations in high performance computing, machine learning and data analytics.</p>
</br>


<h3>Prerequisite</h3>
Experience with C/C++ is expected. Knowledge of parallel algorithms will be helpful, but not required.

<h3> Evaluation </h3>
The evaluation will be based on the following weightings:
<ul>
  <li> 2 pen-and-paper-based assignments (20% weight)  </li>
  <li> 2-3 small programming assignments (30% weight)</li>
  <li> Project (40% weight): Each student will select a topic based on their interests and work on it. The output will be accessed based on a written report and an oral prsentation. </li>
</ul>




 <h3> Recommended reading (evolving) </h3>
  <menu>
 <li> I/O-Optimal Algorithms for Symmetric Linear Algebra Kernels <br>
 <i> Olivier Beaumont, Lionel Eyraud-Dubois, Julien Langou, Mathieu Vérité </i><br>
 ACM Symposium on Parallelism in Algorithms and Architectures, July 2022, Pages 423–433,
 <a href="https://doi.org/10.1145/3490148.35385"> pdf</a>.</li>
  </menu>

  <menu>
    <li>Minimizing Communication in Numerical Linear Algebra <br>
    <i> Grey Ballard, James Demmel, Olga Holtz, Oded Schwartz</i><br>
    SIAM Journal on Matrix Analysis and Applications, Volume 32, Issue 3, pp. 866-901, 2011, <a href="https://doi.org/10.1137/090769156"> pdf</a>.
   </li>
  </menu>



 <menu>
   <li> Tensor Decompositions and Applications <br>
    <i> Tamara G. Kolda and Brett W. Bader </i><br>
    SIAM Review, Volume 51, Number 3, pp. 455-500, 2009, <a href="https://doi.org/10.1137/07070111X"> pdf</a>.</li>
 </menu>
<!--
 <menu>
   <li> Tensor-Train Decomposition <br>
    <i> I. V. Oseledets </i><br>
    SIAM Journal on Scientific Computing, Volume 33, Number 5, pp. 2295-2317, 2011, <a href="https://doi.org/10.1137/090752286"> pdf</a>.</li>
 </menu>



 <menu>
   <li> Communication Lower Bounds and Optimal Algorithms for Multiple Tensor-Times-Matrix Computation <br>
    <i> Hussam Al Daas, Grey Ballard, Laura Grigori, Suraj Kumar, Kathryn Rouse </i><br>
    Available on arxiv, <a href="https://arxiv.org/abs/2207.10437"> pdf</a>.</li>
 </menu>
-->

<h3> Related interesting articles </h3>
<ul>

  <li> <a href="https://arxiv.org/abs/2108.09337">On the Parallel I/O Optimality of Linear Algebra Kernels: Near-Optimal Matrix Factorizations</a>. </li>
  <li> <a href="https://link.springer.com/chapter/10.1007/978-3-642-23397-5_10">Communication-Optimal Parallel 2.5D Matrix Multiplication and LU Factorization Algorithms</a>. </li>
  <li> <a href="https://ieeexplore.ieee.org/document/9820702">Sparsity-Aware Tensor Decomposition</a>. </li>
  <li> <a href="https://inria.hal.science/hal-03079236v5">Higher-Order QR with Tournament Pivoting for Tensor Compression</a>. </li>
  <li> <a href="https://arxiv.org/abs/2306.13835">Computron: Serving Distributed Deep Learning Models with Model Parallel Swapping</a>. </li>

</ul>
</body>
</html>
