<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Parallel computations</title>
  <link rel="stylesheet" href="anne.css" />
</head>
<body bgcolor="#ffffff" text="#000000">

<center><h1 class="title">Parallel computations</h1></center>
<center><h3>Instructor: <a href="https://surakuma.github.io/">Suraj Kumar</a>
  (<a href="http://www.ens-lyon.fr/LIP/ROMA">ROMA</a> team, LIP, ENS Lyon), 2025-2026 </h3></center><br><br>

  <p>
    Parallel systems are now ubiquitous to perform large-scale computations. In this course, we will study several popular ways to perform computations on these systems. Initially, we will focus on parallel  computations on CPUs. Later, we will learn to efficiently exploit GPUs in detail. Additionally, We will have guest lectures from experts on related topics.
  </p>

  <h2>Outline of the course</h2>

  <ul>
    <li> Course details and introduction to paralleization (1-2 lectures)</li>
    <li> Parallel computations on CPUs (2-3 lectures) <ul>
       <li> Shared memory parallelization with OpenMP </li>
       <li> Distributed memory parallelization with MPI </li>
     </ul> </li>

    <li> Parallel computations on GPUs with CUDA (5-6 lectures) </li>
    <li> Exploratory topics in parallel computations and a review of state-of-the-art research articles (3-4 lectures) </li>
    <li> Guest lectures from experts (2-3 lectures) </li>
  </ul>

<!-- <br><br> -->
<br>
 <p>We will also look at several interesting research projects in the course
 related to parallel computations in high performance computing, machine learning, data analytics, molecular simulations and quantum computing.</p>
<br>

<h3>Prerequisite</h3>
Experience with C/C++ is expected. Knowledge of parallel algorithms will be helpful, but not required.

<h3> Evaluation </h3>
The evaluation will be based on the following weightings:
<ul>
  <li> 5-6 small programming assignments (35% weight)</li>
  <li> A mid-term exam (25% weight)</li>
  <li> Project (40% weight): Each student will select a topic based on their interests and work on it. The output will be accessed based on a written report and an oral prsentation. </li>
</ul>

<!--

 <h3> Recommended reading (evolving) </h3>
  <menu>
 <li> Tight Memory-Independent Parallel Matrix Multiplication Communication Lower Bounds <br>
 <i> Hussam Al Daas, Grey Ballard, Laura Grigori, Suraj Kumar, Kathryn Rouse </i><br>
 ACM Symposium on Parallelism in Algorithms and Architectures, July 2022, Pages 445â€“448,
 <a href="https://doi.org/10.1145/3490148.3538552"> Short version</a>,
 <a href="https://arxiv.org/abs/2205.13407"> Extended version</a>.</li>
  </menu>

  <menu>
    <li>Minimizing Communication in Numerical Linear Algebra <br>
    <i> Grey Ballard, James Demmel, Olga Holtz, Oded Schwartz</i><br>
    SIAM Journal on Matrix Analysis and Applications, Volume 32, Issue 3, pp. 866-901, 2011, <a href="https://doi.org/10.1137/090769156"> pdf</a>.
   </li>
  </menu>



 <menu>
   <li> Tensor Decompositions and Applications <br>
    <i> Tamara G. Kolda and Brett W. Bader </i><br>
    SIAM Review, Volume 51, Number 3, pp. 455-500, 2009, <a href="https://doi.org/10.1137/07070111X"> pdf</a>.</li>
 </menu>
 <menu>
   <li> Tensor-Train Decomposition <br>
    <i> I. V. Oseledets </i><br>
    SIAM Journal on Scientific Computing, Volume 33, Number 5, pp. 2295-2317, 2011, <a href="https://doi.org/10.1137/090752286"> pdf</a>.</li>
 </menu>



 <menu>
   <li> Communication Lower Bounds and Optimal Algorithms for Multiple Tensor-Times-Matrix Computation <br>
    <i> Hussam Al Daas, Grey Ballard, Laura Grigori, Suraj Kumar, Kathryn Rouse </i><br>
    Available on arxiv, <a href="https://arxiv.org/abs/2207.10437"> pdf</a>.</li>
 </menu>
-->


</body>
</html>
