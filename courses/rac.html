<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Resource-aware computations on CPUs and GPUs</title>
  <link rel="stylesheet" href="anne.css" />
</head>

<body bgcolor="#ffffff" text="#000000">

<center><h1 class="title">Resource-aware computations on CPUs and GPUs</h1></center>
<center><h3>Instructors: <a href="https://surakuma.github.io">Suraj Kumar</a>, <a href="http://perso.ens-lyon.fr/loris.marchal">Loris Marchal</a> & <a href="http://perso.ens-lyon.fr/frederic.vivien">Frédéric Vivien</a>
  (<a href="http://www.ens-lyon.fr/LIP/ROMA">ROMA</a> team, LIP, ENS Lyon), 2025-2026 </h3></center><br><br>

<!--
<h1>Data-aware parallel algorithms for matrix and tensor computations</h1>

<h4>Instructor: <a href="https://surakuma.github.io/">Suraj Kumar</a>, Fall 2023 </h4>

<i>Course notes and slides:</i>
-->
<p>
  Computing platforms have limited resources, such as memory, cache, bandwidth, and processing power. In the past, algorithms have been derived with optimal complexity and thus are supposed to make an efficient usage of processing power, while memory, cache and bandwidth limitations have often be ignored. However, it is often the case that these resources become the one limiting the overall performance, especially (but not only) when using accelerators such as GPUs. These GPUs offer increased processing capabilities and superior energy efficiency compared to CPUs, making them a crucial element of many computing systems over the past decade.
</p>

<p>
  In this course, we will present on one hand algorithmic approaches that have recently been proposed in order to utilize all resources efficiently, and on the other hand we will focus on how to implement these efficient algorithms on real hardware platforms. The typical use case will focus on linear algebra computations (matrix operations), which are the basis of both "traditional" high performance computing applications and recent neural network computations.
</p>
<!--
<p>

Computing platforms have limited resources, such as cache, bandwidth, and processing power. Therefore, it is important to utilize these resources efficiently, which also helps to reduce energy consumption. GPUs offer increased processing capabilities and superior energy efficiency compared to CPUs, making them a crucial element of many computing systems over the past decade.</p>

<p>
  The goal of this course is to present approaches to utilize resources efficiently for linear algebra (matrix) computations. Initially, we will focus on computations on CPUs. Later, we will learn to efficiently exploit GPUs.
</p>
-->
<div class="row" style="display: flex; justify-content: space-around; align-items: center;">
  <figure style="text-align: center;">
    <img src="figs/sequential-machine.png" alt="seq-machine" style="width: 100%;">
    <figcaption>A sequential machine.</figcaption>
  </figure>&nbsp;
  <figure style="text-align: center;">
    <img src="figs/distributed-machine.png" alt="distributed-machine" style="width: 100%;">
    <figcaption>A distributed memory machine.</figcaption>
  </figure>&nbsp;
  <figure style="text-align: center;">
    <img src="figs/V100-architecture.png" alt="V100-GPU" style="width: 60%;">
    <figcaption>Nvidia Volta 100 architecture.</figcaption>
  </figure>
</div>



<h2>Outline</h2>
<ul>
  <li> Introduction <ul>
    <li> Course details and evaluation methods</li>
    <li> Relevance of this course</li>
  </ul></li>
  <li> Computations on CPUs <ul>
      <li> Matrix multiplications</li>
      <li> I/O complexity bounds </li>
      <li> Data transfer cost models </li>
      <li> Communication avoiding sequential and parallel algorithms </li>
      <li> 2D/2.5D/3D algorithms
      <li> Directed acyclic graph (DAG) scheduling
      <li> Memory aware computations </li>
      <li> Exploratory research topics </li>
    </ul></li>

  <li> Computations on GPUs <ul>
    <li> GPU vs CPU architecture </li>
    <li> CUDA basics </li>
    <li> Memory model, threads, blocks </li>
    <li> Memory optimizations using shared memory </li>
    <li> Coalescing and memory alignment </li>
    <li> Tensor cores </li>
    <li> Exploratory research topics </li>
  </ul>
  </li>
  <li> Implementation on CPUs and GPUs <ul>
    <li> Parallelization using OpenMP and MPI </li>
    <li> Parallelization using CUDA </li>
  </ul>
  </li>
</ul>



<br>
 <p>We will look at several interesting research projects in the course
 related to parallel computations in high performance computing, machine learning and data analytics.</p>
</br>


<h3>Prerequisite</h3>
Experience with C/C++ is expected. Knowledge of parallel algorithms will be helpful, but not required.

<h3> Evaluation </h3>
The evaluation will be based on the following weightings:
<ul>
  <li> 2 pen-and-paper-based assignments (20% weight)  </li>
  <li> 2-3 small programming assignments (30% weight)</li>
  <li> Project (50% weight): Each student will select a topic based on their interests and work on it. The output will be accessed based on a written report and an oral prsentation. </li>
</ul>




 <h3> Recommended reading (evolving) </h3>
  <menu>
 <li> I/O-Optimal Algorithms for Symmetric Linear Algebra Kernels <br>
 <i> Olivier Beaumont, Lionel Eyraud-Dubois, Julien Langou, Mathieu Vérité </i><br>
 ACM Symposium on Parallelism in Algorithms and Architectures, July 2022, Pages 423–433,
 <a href="https://doi.org/10.1145/3490148.35385"> pdf</a>.</li>
  </menu>

  <menu>
    <li>Minimizing Communication in Numerical Linear Algebra <br>
    <i> Grey Ballard, James Demmel, Olga Holtz, Oded Schwartz</i><br>
    SIAM Journal on Matrix Analysis and Applications, Volume 32, Issue 3, pp. 866-901, 2011, <a href="https://doi.org/10.1137/090769156"> pdf</a>.
   </li>
  </menu>


</body>
</html>
