<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>Data-aware algorithms for matrix and tensor computations</title>
  <link rel="stylesheet" href="anne.css" />
</head>
<body bgcolor="#ffffff" text="#000000">

<center><h1 class="title">Data-aware algorithms for matrix and tensor computations</h1></center>
<center><h3>Instructor: <a href="https://surakuma.github.io/">Suraj Kumar</a>
  (<a href="http://www.ens-lyon.fr/LIP/ROMA">ROMA</a> team, LIP, ENS Lyon), 2024-2025 </h3></center><br><br>

<!--
<h1>Data-aware parallel algorithms for matrix and tensor computations</h1>

<h4>Instructor: <a href="https://surakuma.github.io/">Suraj Kumar</a>, Fall 2023 </h4>

<i>Course notes and slides:</i>
-->


<p>
In the last few decades, the rate of operations of computing systems has improved
drastically but we have noticed relatively smaller improvement in the rate of
data transfers. The current computing systems face bottleneck due to the large
volume of data transfers. One of the promising approaches to solve  this problem
is to design algorithms which minimize the data transfers. The goal of this course
is to present a variety of such algorithms for matrix and tensor computations.
</p>

<p> Tensors are multi-dimensional arrays and arise in several domains, e.g., data
mining, neurosciences, quantum computing, molecular dynamics and computer vision.
<!--The course will also discuss how tensors are used to analyze multi-dimensional
data sets. -->
The course will present several tensor computations in detail.
<!--also present extension of data-aware approaches of matrix computations
for tensors.-->
</p>

<div>
<figure>
<img width="500" src="figs/TuckerDecomposition.png" class="center" alt="Tucker decomposition example"/>
<figcaption text-align: center>
&emsp;&emsp; Tucker decomposition of a 3-dimensional tensor.
</figcaption>
</figure>
</div>
<br>

<p>
  The course will also discuss various approaches to compute communication lower
  bounds (that determine how much data transfers are required) for different matrix
  and tensor computations. Establishing communication lower bounds helps one to
  identify efficient algorithms that exactly attain the bounds.</p>





<h2>Part 1: Communication lower bounds and optimal algorithms for matrix computations</h2>
<p> In this part, we will first study the seminal work of Hong and Kung (from 1960s)
  to determine the minimum data transfer between levels of a memory hierarchy on a
  sequential machine (a single processor).
  After that, we will review other approaches to obtain communication lower bounds on sequential
  and parallel systems. We will also study some algorithms for both types of systems which attain the
  lower bounds.</p>

  <ul>
    <li> Matrix multiplications</li>
    <li> Pebble games </li>
    <li> Data transfer cost models </li>
    <li> Memory dependent and independent communication lower bounds
    <li> Communication avoiding sequential and parallel algorithms </li>
    <li> 2D/2.5D/3D algorithms
    <li> Strassen's matrix multiplications
    <li> Matrix factorizations </li>
  </ul>

<h2>Part 2: Tensor computations</h2>
<p> In this part, we will first look at tensor notations and different types of popular
tensor operations. After that, we will study widely used tensor decompositions and
different algorithms to compute them. We will also analyze the bottleneck computations
of various tensor decomposition algorithms and discuss parallel and communication
optimal approaches for those computations.</p>
<ul>
  <li> Tensor notations </li>
  <li> Use of tensors in data analytics and quantum computing</li>
  <li> Tensor decompositions: Canonical Polydiac, Tucker, Tensor train </li>
  <li> Matricized tensor times Khatri-Rao product computations </li>
  <li> Multiple tensor times matrix computations </li>
</ul>

<h2>Part 3: Guest lectures on related popular topics</h2>
 <p>We will have some lectures on related topics from experts.</p>
 <ul>
   <!-- <li> Memory-aware scheduling </li> -->
   <li> Sparse matrix and tensor computations </li>
   <li> Hierarchical and block low-rank matrices </li>
   <li> Randomized methods to improve algorithmic costs </li>
 </ul>
<!-- <br><br> -->
<br>
 <p>We will also look at several interesting research projects in the course
 related to matrix/tensor computations in high performance computing, machine learning,
 data analytics, molecular simulations and quantum computing.</p>
<br>

<h3>Prerequisite</h3>
Experience with algorithm analysis and mathematical optimization will be helpful,
but not required.

<h2> Lecture notes </h2>
<ul>
  <li> Lecture 1 (Sep 10): Overview of the course <a href="../CR12-2024/Lecture1/course_overview.pdf">slides</a>
      and sequential matrix multiplication <a href="../CR12-2024/Lecture1/seqmatmul.pdf">slides</a>.
      Suggested reading <a href="https://arxiv.org/abs/1702.02017">pdf</a>.
      Assignment 1 is out with two questions (please see the slides), and it is due by Sep 19. </li>
    <li> Lecture 2 (Sep 12): Costs of some MPI collective routines <a href="../CR12-2024/Lecture2/mpiCollectiveCommunication.pdf">slides</a>.</li>
    <li> Lecture 3 (Sep 17): Parallel matrix multiplication <a href="../CR12-2024/Lecture3/parallelMatmul.pdf">slides</a>. Suggested reading <a href="https://arxiv.org/abs/2205.13407">pdf</a>. Assignment 2 is out (please see the slides), and it is due by Sep 26. </li>
    <li> Lecture 4 (Sep 19): Symmetric matrix computations <a href="../CR12-2024/Lecture4/symmetricComputations.pdf">slides</a>. Suggested reading <a href="https://inria.hal.science/hal-04076513v1">pdf1</a>, <a href="https://arxiv.org/abs/2409.11304">pdf2</a>. </li>
    <li> Lecture 5 (Sep 24): Matrix factorizations <a href="../CR12-2024/Lecture5/matrixFactorizations.pdf"> slides</a>. Suggested reading <a href="https://doi.org/10.1137/090769156">pdf</a>.  </li>
    <li> Lecture 6 (Sep 26): Remaining portions of matrix factorizations. </li>
    <li> Lecture 7 (Oct 01): Introduction to tensors <a href="../CR12-2024/Lecture7/tensors.pdf">slides</a>. Suggested reading <a href="https://www.kolda.net/publication/TensorReview.pdf">pdf</a>. Assignment 3 is out (please see the slides), and  it is due by Oct 10. If the unfoldings of the specified tensor are not vey clear, please have a look at all the three unfoldings <a href="../CR12-2024/Lecture7/assignment3.pdf">here</a>. </li>
    <li> Lectures 8-9 (Oct 03 & Oct 8): Low rank approximations of tensors <a href="../CR12-2024/Lecture8/tensorDecompositions.pdf">slides</a>. </li>
    <li> Lecture 10 (Oct 10): Implementation of sequential LU and Householder QR factorizations <a href="../CR12-2024/Lecture10/hqrg.py">qr_file</a>. </li>
    <li> Lecture 11 (Oct 15): Matricized tensor times Khatri-Rao product computations <a href="../CR12-2024/Lecture11/MTTKRP.pdf">slides</a>. Suggested reading <a href="https://arxiv.org/abs/1708.07401">pdf</a>. </li>
    <li> Lecture 12 (Oct 17): It will be on Multiple tensor times matrix computations. </li>
  </ul>
  <!--
  <span style="color:green"><strong> sdf  </strong></span>
  <li> Lecture 2 (Sep 14): Costs of some MPI routines <a href="../CR12/Lecture2/mpiCollectiveCommunication.pdf">slides</a>.
    A C++ program that practically analyzes all loop permutations for the sequential matrix
    multiplication <a href="../CR12/matrixpermutation.cpp">code</a>.</li>
    <li> Lecture 3 (Sep 19): Parallel matrix multiplication <a href="../CR12/Lecture3/parallelMatmul.pdf">slides</a>. Suggested reading <a href="https://arxiv.org/abs/2205.13407">pdf</a>.
      Assignment 2 is out (please see the slides), and it is due by Sep 28.  </li>
    <li> Lecture 4 (Sep 21): Matrix factorizations <a href="../CR12/Lecture4/matrixFactorizations.pdf"> slides</a>. Suggested reading <a href="https://doi.org/10.1137/090769156">pdf</a>.  </li>
    <li> Lecture 5 (Sep 26): Symmetric computations <a href="../CR12/Lecture5/symmetricComputations.pdf">slides</a>. Suggested reading <a href="https://inria.hal.science/hal-04076513v1">pdf</a>.
      Assignment 3 is out (please see the slides), and it is due by Oct 5.  </li>
    <li> Lecture 6 (Sep 28): Introduction to tensors <a href="../CR12/Lecture6/tensors.pdf">slides</a>.
      Suggested reading <a href="https://www.kolda.net/publication/TensorReview.pdf">pdf</a>. Assignment 4 is out (please see the slides),
      and it is due by Oct 10. If the unfoldings of the specified tensor are not vey clear, please have a look at all
      the three unfoldings <a href="../CR12/Lecture6/assignment4.pdf">here</a>. </li>
    <li> Lecture 7 (Oct 3): Low rank approximations of tensors <a href="../CR12/Lecture7/tensorDecompositions.pdf">slides</a>.
      Suggested reading for Tensor train decomposition <a href="https://sites.pitt.edu/~sjh95/related_papers/tensor_train_decomposition.pdf">pdf</a>.
      The research topics/articles for the course project are out (please see the slides and inform me your choices). </li>
    <li> Lecture 8 (Oct 5): Guest lecture by <a href="http://perso.ens-lyon.fr/loris.marchal/">Loris Marchal</a> on <em> Memory-aware scheduling </em>
      <a href="../CR12/Lecture8/MemoryAwareDAGScheduling.pdf">slides</a>. </li>
    <li> Lecture 9 (Oct 10): Guest lecture by <a href="http://perso.ens-lyon.fr/gregoire.pichon">Grégoire Pichon</a> on <em> Low-Rank compression in sparse
      solvers </em> <a href="../CR12/Lecture9/LowRankCompressionSparseSolvers.pdf">slides</a>.</li>
    <li> Lecture 10 (Oct 12): Matricized tensor times Khatri-Rao product computations <a href="../CR12/Lecture10/MTTKRP.pdf">slides</a>.
      Suggested reading <a href="https://arxiv.org/abs/1708.07401">pdf</a>. </li>
    <li> Lecture 11 (Oct 17): Multiple tensor times matrix computations <a href="../CR12/Lecture11/Multi-TTM.pdf">slides</a>.
        Suggested reading <a href="https://inria.hal.science/hal-03950359">pdf</a>. Final assignment is out (please see the slides), and it is due by Oct 26.
        </li>
    <li> Lecture 12 (Oct 19): Guest lecture by <a href="http://perso.ens-lyon.fr/bora.ucar">Bora Uçar</a> on <em> sparse matrices and
      bipartite graphs </em> <a href="../CR12/Lecture12/buBVN.pdf">slides</a>. </li>
    <li> Lecture 13 (Oct 24): Remaining portions of Multiple tensor times matrix computations. </li>
    <li> Lecture 14 (Nov 7):  Implementation of CP/Tucker/Tensor train decompositions in Matlab/Octave <a href="../CR12/Lectures13-14/tensordecompositions.tar.gz">files</a>. </li>
    <li> Lectures 15-16 (Nov 9): Project presentations and discussion. </li>
</ul>

-->
<br>
Acknowledgement: I am extremely thankful to Grey Ballard and Loris Marchal for sharing source files of slides of their courses. I am using some of their slides in this course.

<h3> Evaluation </h3>
The evaluation will be based on the following weightings:
<ul>
  <li> 4 homework assignments (best 3 would be counted): 40%</li>
  <li> Project (60%): A list of selected research topics/papers will be provided.
    Each student or a group of 2 students will prepare a report and give a presentation for one of these.</li>

    <!--Each student has to analyze contributions of one of the given research
    papers. The output will be assessed based on a written report (in ACM
    format: <a href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>)
    and an oral presentation.-->
</ul>
 <h3> Course project for the final evaluation </h3>
 Each student or a group of two students will prepare a 5-6 pages report and give a presentation for one of the below topics/articles. Project report is due by Nov 5.
 <!--
   <span style="color:green"><strong> Project report is due by Nov 5.</strong></span>
   <li> A parallel method to perform MTTKRP on a parallel shared memory machine. <span style="color:green"><strong>Presentation on Nov 9 at 15h10 by Adrien Obrecht and Enrique Galvez.</strong></span> <a href="../CR12-2024/Project-articles/SPLATT.pdf">SPLATT: Efficient and Parallel Sparse Tensor-Matrix Multiplication</a> </li>
-->
<!--
-->

 <span style="color:brown"><strong> Email me your or your group choice (or propose another topic/article) by Oct 15 </strong></span>.



 <h4> Research topics </h4>
 <ul>
   <li> Communication costs of a specific matrix factorization (LU, QR, TSQR, RRQR, ...)</li>
   <li> Extending a specific matrix factorization for tensors </li>
   <li> Use of tensors in a particular domain, for example, neuroscience, data analysis, molecular simulations, quantum computing, face recognition, ... </li>
 </ul>
 <h4> Research articles </h4>
 <ul>
   <li> Communication lower bounds for several computations on a sequential machine. <a href="../CR12-2024/Project-articles/AutomaticLowerBounds.pdf">Automated Derivation of Parametric Data Movement Lower Bounds for Affine Programs</a> </li>
   <li> Performance optimizations for TSQR algorithm. <a href="../CR12-2024/Project-articles/TSQROptimizations.pdf">Reconstructing Householder Vectors from Tall-Skinny QR</a> </li>
   <li> Low rank approximation for stencil computations. <a href="../CR12-2024/Project-articles/LoRAStencil.pdf">LoRAStencil: Low-Rank Adaptation of Stencil Computation on Tensor Cores</a>  </li>
   <li> Sequential lower bounds and optimal algorithms for symmetric computations. <a href="../CR12-2024/Project-articles/SequentialSYRK.pdf">I/O-Optimal Algorithms for Symmetric Linear Algebra Kernels</a>  </li>
   <li> Hypergraph partitioning-based methods to improve MTTKRP performance. <a href="../CR12-2024/Project-articles/SparseMTTKRP.pdf">Scalable Sparse Tensor Decompositions in Distributed Memory Systems</a>  </li>
   <li> A parallel method to perform MTTKRP on a parallel shared memory machine. <span style="color:green"><strong>Assigned to Justine Cauvi.</strong></span> <a href="../CR12-2024/Project-articles/SPLATT.pdf">SPLATT: Efficient and Parallel Sparse Tensor-Matrix Multiplication</a> </li>
   <li> Randomization based parallel HOSVD and ST-HOSVD methods. <a href="../CR12-2024/Project-articles/RandomizedTuckerDecomposition.pdf">Parallel Randomized Tucker Decomposition Algorithms</a>  </li>
   <li> Tucker decomposition to improve performance of convolution kernels. <a href="../CR12-2024/Project-articles/TuckerDecompositionCNN.pdf">Stable Low-rank Tensor Decomposition for Compression of Convolutional Neural Network</a>  </li>
   <li> Tensor train representation for the weight matrices of the fully connected layers. <a href="../CR12-2024/Project-articles/TensorizingNeuralNetworks.pdf">Tensorizing Neural Networks</a>  </li>
   <li> Use of tensor train representation in quantum systems. <a href="../CR12-2024/Project-articles/DMRG.pdf">The density-matrix renormalization group: a short introduction</a>  </li>
 </ul>


 <br>
 If you want to propose another topic/article, I would be happy to discuss it with you.


 <h3> Recommended reading (evolving) </h3>
  <menu>
 <li> Tight Memory-Independent Parallel Matrix Multiplication Communication Lower Bounds <br>
 <i> Hussam Al Daas, Grey Ballard, Laura Grigori, Suraj Kumar, Kathryn Rouse </i><br>
 ACM Symposium on Parallelism in Algorithms and Architectures, July 2022, Pages 445–448,
 <a href="https://doi.org/10.1145/3490148.3538552"> Short version</a>,
 <a href="https://arxiv.org/abs/2205.13407"> Extended version</a>.</li>
  </menu>

  <menu>
    <li>Minimizing Communication in Numerical Linear Algebra <br>
    <i> Grey Ballard, James Demmel, Olga Holtz, Oded Schwartz</i><br>
    SIAM Journal on Matrix Analysis and Applications, Volume 32, Issue 3, pp. 866-901, 2011, <a href="https://doi.org/10.1137/090769156"> pdf</a>.
   </li>
  </menu>



 <menu>
   <li> Tensor Decompositions and Applications <br>
    <i> Tamara G. Kolda and Brett W. Bader </i><br>
    SIAM Review, Volume 51, Number 3, pp. 455-500, 2009, <a href="https://doi.org/10.1137/07070111X"> pdf</a>.</li>
 </menu>
<!--
 <menu>
   <li> Tensor-Train Decomposition <br>
    <i> I. V. Oseledets </i><br>
    SIAM Journal on Scientific Computing, Volume 33, Number 5, pp. 2295-2317, 2011, <a href="https://doi.org/10.1137/090752286"> pdf</a>.</li>
 </menu>



 <menu>
   <li> Communication Lower Bounds and Optimal Algorithms for Multiple Tensor-Times-Matrix Computation <br>
    <i> Hussam Al Daas, Grey Ballard, Laura Grigori, Suraj Kumar, Kathryn Rouse </i><br>
    Available on arxiv, <a href="https://arxiv.org/abs/2207.10437"> pdf</a>.</li>
 </menu>
-->

<h3> Related interesting articles </h3>
<ul>

  <li> <a href="https://arxiv.org/abs/2108.09337">On the Parallel I/O Optimality of Linear Algebra Kernels: Near-Optimal Matrix Factorizations</a>. </li>
  <li> <a href="https://link.springer.com/chapter/10.1007/978-3-642-23397-5_10">Communication-Optimal Parallel 2.5D Matrix Multiplication and LU Factorization Algorithms</a>. </li>
  <li> <a href="https://ieeexplore.ieee.org/document/9820702">Sparsity-Aware Tensor Decomposition</a>. </li>
  <li> <a href="https://inria.hal.science/hal-03079236v5">Higher-Order QR with Tournament Pivoting for Tensor Compression</a>. </li>
  <li> <a href="https://arxiv.org/abs/2306.13835">Computron: Serving Distributed Deep Learning Models with Model Parallel Swapping</a>. </li>

</ul>
</body>
</html>
