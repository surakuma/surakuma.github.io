\documentclass[a4paper,11pt]{article}

\usepackage[light,math]{iwona}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[american]{babel}
\usepackage{amsfonts,amsmath,wasysym} %% Additional math chars
\usepackage{marvosym}
\usepackage{eurosym}

\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{eurosym}

\usepackage{etoolbox}
\patchcmd\section{2.3ex}{1.8ex}{}{}

%%\usepackage[backend=bibtex,maxbibnames=99,
%%	sortcites=true,
%%	doi=false,url=false]{biblatex}
%\addbibresource{submission.bib}
\usepackage{tikz}
\usepackage{tikzscale}
\usetikzlibrary{matrix}
\usetikzlibrary{fit}
\usetikzlibrary{trees}
\usetikzlibrary{backgrounds}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}
\usetikzlibrary{trees}
\usetikzlibrary{positioning}
\usepackage{mdframed}


\input{./tensor_header}
\newcommand{\X}{\T{X}}
\newcommand{\Y}{\T{Y}}

\definecolor{pastelgreen1}{rgb}{0.47, 0.87, 0.47}
\definecolor{pastelgreen}{rgb}{0, 1, 0}

\definecolor{orange1}{RGB}{242, 142, 30}
\definecolor{orange2}{RGB}{245, 165, 77}
\definecolor{blue1}{RGB}{43, 114, 178}
\definecolor{blue2}{RGB}{76, 147, 212}
\usepackage[tight,footnotesize]{subfigure}

\usepackage[bookmarks=true,
bookmarksnumbered=true,
bookmarksopen=false,
plainpages=false,
pdfpagelabels,
colorlinks]{hyperref}

\definecolor{anrblue}{rgb}{0,0.2,0.4}
\definecolor{anrviolet}{rgb}{0.5,0,0.5}

\definecolor{pdfurlcolor}{rgb}{0,0,0.6}
\definecolor{pdfcitecolor}{rgb}{0,0.6,0}
\definecolor{pdflinkcolor}{rgb}{0.6,0,0}
%%
%%\usepackage[colorlinks=true,citecolor=pdfcitecolor,urlcolor=pdfurlcolor,linkcolor=pdflinkcolor,pdfborder={0 0 0}]{hyperref}


%%\hypersetup{
%%	citecolor=anrblue,              % color of cite links
%%	pagecolor=anrblue,         % color of page links
%%	menucolor=anrblue,         % color of Acrobat Reader menu buttons
%%	urlcolor=anrblue,       % color of page of \url{...}
%%	linkcolor=anrblue,
%%	urlbordercolor=red,% hyperlink borders will be red
%%	pdfborderstyle={/S/U/W 1}% border style will be underline of width  % 1pt
%%}

\hypersetup{
	citecolor=pdfcitecolor,              % color of cite links
	pagecolor=anrblue,         % color of page links
	menucolor=anrblue,         % color of Acrobat Reader menu buttons
	urlcolor=pdfurlcolor,       % color of page of \url{...}
	linkcolor=pdflinkcolor,
	urlbordercolor=red,% hyperlink borders will be red
%%	pdfborderstyle={/S/U/W 1}% border style will be underline of width  % 1pt
}

\makeatletter
\Hy@AtBeginDocument{%
	\def\@pdfborder{0 0 1}% Overrides border definition set with colorlinks=true
	\def\@pdfborderstyle{/S/U/W 0.5}% Overrides border style set with colorlinks=true
	% Hyperlink border style will be underline of width 1pt
}
\makeatother

\ifxetex
\usepackage{fontspec}
\usepackage{fontawesome}
\defaultfontfeatures{Mapping=TeX-text}
\defaultfontfeatures{Ligatures=NoCommon}
\let\sfdefault\rmdefault
\setmainfont{FreeSerif}
\setromanfont{FreeSerif}
\newfontfamily\sectionfont[Color=anrblue]{DejaVu Sans}
\newfontfamily\subsectionfont[Color=anrblue]{DejaVu Sans}
\newfontfamily\tocsectionfont[Color=anrblue]{DejaVu Sans}
\newcommand\linksym{\faExternalLink}
\else
\newcommand\sectionfont{\color{anrblue}\normalfont\fontsize{14}{14}\bfseries}
\newcommand\subsectionfont{\color{anrblue}\normalfont\fontsize{12}{12}\bfseries}
\newcommand\tocsectionfont{\color{anrblue}\normalfont}
\newcommand\linksym{\Mundus}
\fi


\usepackage{color,xspace,paralist}
\definecolor{blue}{rgb}{0,0,1}
\newcommand{\blue}[1]{{\color{blue} #1}}

% Comment/uncomment the next lines to remove the guidelines

\newcommand{\gl}[1]{{\color{blue} \emph{#1}}}
\renewcommand{\gl}[1]{}
\newcommand{\bora}[1]{{\color{magenta} \emph{#1}}}
%\renewcommand{\bora}[1]{}
\newcommand{\sk}[1]{{\color{blue} \emph{#1}}}
%\renewcommand{\sk}[1]{}

\newcommand{\todo}[1]{{\color{red}\rule[-.1cm]{.4cm}{.4cm}~~{
			\color{red}{TODO: #1}}}\xspace}


\newcommand{\link}[2]{\href{#2}{#1\linksym}\xspace}


\newcommand*{\titleSK}{\begingroup 
	\centering 
	\begin{mdframed}[roundcorner=10pt,linewidth=1pt,leftmargin=10,rightmargin=10,shadow=true]%
		\vspace*{0.2\baselineskip}
		\centering
		{\Large \sc SCATE: SCAlable Tensor dEcompositions for data analytics}\\[0.2cm]
		%%        /STINT
		%%        {\LARGE \sc to enhance Low-Rank Compression}\\
		{\large \sc (Décompositions de tenseurs évolutives pour l'analyse de données)}\\[0.1cm]
		{\small CES46 $>$ Axe E.05 $>$ Calcul haute performance, Modèles numériques, simulation, applications}\\
		{\normalsize AAPG 2025 / JCJC 48 months}
%%		{\normalsize AAPG 2023 / JCJC 48 months / Budget 283K\euro\bora{no need for budget here}}
		\vspace*{0.2\baselineskip}
	\end{mdframed}
%%	  \vspace*{0.0cm} 
	\begin{center}
		\vspace*{0.025\baselineskip}
		{\Large \sc Suraj Kumar, {\sc ROMA} team, {\sc LIP, ENS Lyon}}
	\end{center}
	\endgroup}



\title{SCATE: SCAlable Tensor dEcompositions for data analytics}
\author{Suraj Kumar, ROMA team, LIP, ENS Lyon}
\date{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{}
\lhead{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.4pt}
\lfoot{S. Kumar, SCATE}
\rfoot{\thepage}
\cfoot{}


%%%\bibliography{scate}
%
%\begin{document}
%	%\thispagestyle{plain}
%	%\vspace{3cm}
%%	\titleSK
%\maketitle
	

\newcommand{\piname}{Suraj KUMAR}
%\title{Action Exploratoire (AEx) \\
% {\normalsize Pour toute question, contacter : \texttt{aex-contact@inria.fr}} \\
%  {\normalsize Soumission à \texttt{aex-contact@inria.fr}} \\ {\normalsize avec en copie REP, DCR, DS et DS~adjoint.} }
\author{}
\date{}
\begin{document}
	%\maketitle
	
	\begin{center}
		\Large\textbf{Projet émergent}
	\end{center}
	
	%\newcommand{\prname}{LOTUS}
	%\begin{Large}
	\begin{tabular}{rl}
		Project: & {\sc SCAlable Tensor dEcompositions for data analytics (SCATE)}\\
		PI: & Suraj KUMAR, ROMA team, LIP, ENS Lyon\\
	\end{tabular}\newline


%	\section*{Summary}
	\textbf{Summary}: \emph{Tensors are multi-dimensional arrays and used to store data in several domains, e.g., data mining, neuroscience, astrophysics, scientific simulations and computer vision. Tensor decompositions help to identify inherent structure of data, achieve data compression and enable various ways of data analysis. It is paramount to devise parallel tensor decomposition algorithms that effectively utilize modern computing systems. The principal challenges are data transfer costs of tensor operations and limited parallelism exposed by most existing algorithms. This project aims at solving these challenges and proposing new decomposition algorithms which scale well on current and future computing systems. Finally, the proposed algorithms will be implemented for large scale distributed systems and evaluated on real-world tensors from neuroscience and combustion simulations. The implementation will also be used to accelerate hyperspectral data analysis in astrophysics and brain-computer interface data analysis in electrophysiology.}
	
	\vspace*{-0.35cm}\section*{Context, positioning, objectives and budget}\vspace*{-0.05cm}
	
	
	We produce approximately 3.3 quintillion bytes of data every day~\cite{data-size-2024}. To analyze such massive amount of data, we need to design parallel and scalable approaches which can make efficient utilization of modern computing systems. Tensors are multi-dimensional arrays and used to store data in several domains~\cite{KB-SIAM-2009}, e.g., data mining, neuroscience and computer vision. Tensor decompositions help to identify inherent structure of data, achieve data compression and enable various ways of data analysis.
%	Tensors are also used as operators to solve problems in applied mathematics, chemistry, physics, machine learning and many other fields~\cite{KB-SIAM-2009,NPOV-NIPS-2015}. 
	Working with tensors is challenging as the computational and memory requirements grow exponentially in the number of dimensions. It is therefore necessary to work with patterns of the tensor data. Using low dimensional structure of high dimensional data is a powerful approach in this context. Most tensor decompositions represent data in its low dimensional structures.
	
	

%%\sk{History of CP: Polyadic (1927)-->Canonical decomposition/PARAFAC (1970) --> CP(2000) --> Canonical Polyadic (2010)}	
%%	\bora{I think Canonical polyadic (or CP) is the shorted form; I do not know if the word ``canonical'' is a name on its own} 
	CP (also known as Canonical Polyadic or CANDECOMP or PARAFAC) and Tucker are the widely used tensor decompositions in the literature for data analytics. Both decompositions can be viewed as high order generalization of Singular Value Decomposition (SVD). CP decomposition is used to understand the latent components of the data. Tucker decomposition is considered to be more appropriate for compression and multi-modal data analysis.
	
	\vspace*{-0.15cm}\begin{figure}[htb]
		\begin{center}
			\subfigure[CP decomposition.]{
				\begin{tikzpicture}[scale=0.23, every node/.style={transform shape}]
				
				\path (-2,0,-7) -- (2,0,7);
				
				\pgfmathsetmacro{\cubex}{4}
				\pgfmathsetmacro{\cubey}{4}
				\pgfmathsetmacro{\cubez}{4}
				\draw[blue,fill=pastelgreen] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
				\draw[blue,fill=pastelgreen] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
				
				\node[draw=none, text=black, scale=4] at (2,-2.25,-3) {$=$};
				\pgfmathsetmacro{\smallwidth}{0.5}
				\draw[blue,fill=pastelgreen] (\cubex+2,0,0) -- ++(-\smallwidth,0,0) -- ++(0,-\cubey,0) -- ++(\smallwidth,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (\cubex+2 +\cubex + 0.5,0.75,0) -- ++(-\cubex,0,0) -- ++(0,-\smallwidth,0) -- ++(\cubex,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (\cubex+2,0.5,0) -- ++(-\smallwidth,0,0) -- ++(0,0,-\cubez) -- ++(\smallwidth,0,0) -- cycle;
				
				\node[draw=none, text=black, scale=4] at (2+\cubex+3.8,-2.25,-3) {$+$};
				
				\draw[blue,fill=pastelgreen] (\cubex+2.5 + \cubex+2,0,0) -- ++(-\smallwidth,0,0) -- ++(0,-\cubey,0) -- ++(\smallwidth,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (\cubex+2.5+\cubex+2 +\cubex + 0.5,0.75,0) -- ++(-\cubex,0,0) -- ++(0,-\smallwidth,0) -- ++(\cubex,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (\cubex+2.5+\cubex+2,0.5,0) -- ++(-\smallwidth,0,0) -- ++(0,0,-\cubez) -- ++(\smallwidth,0,0) -- cycle;
				
				\node[draw=none, text=black, scale=4] at (2+\cubex+5 + \cubex+ 4.25, -2.25,-3) {$+$ $\cdots$ $+$};
				
				\draw[blue,fill=pastelgreen] (12 + \cubex+2.5 + \cubex+2,0,0) -- ++(-\smallwidth,0,0) -- ++(0,-\cubey,0) -- ++(\smallwidth,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (12+\cubex+2.5+\cubex+2 +\cubex + 0.5,0.75,0) -- ++(-\cubex,0,0) -- ++(0,-\smallwidth,0) -- ++(\cubex,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (12 + \cubex+2.5+\cubex+2,0.5,0) -- ++(-\smallwidth,0,0) -- ++(0,0,-\cubez) -- ++(\smallwidth,0,0) -- cycle;
				\end{tikzpicture}}$\qquad\qquad$
			\subfigure[Tucker decomposition.]{
				\begin{tikzpicture}[scale=0.23, every node/.style={transform shape}]
				\pgfmathsetmacro{\cubex}{4}
				\pgfmathsetmacro{\cubey}{4}
				\pgfmathsetmacro{\cubez}{4}
				\draw[blue,fill=pastelgreen] (-12,1,\cubez-2) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (-12,1,\cubez-2) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
				\draw[blue,fill=pastelgreen] (-12,1,\cubez-2) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
				\node[draw=none, text=black, scale=4] at (-8,-1,0) {$=$};
				
				\pgfmathsetmacro{\cubex}{2}
				\pgfmathsetmacro{\cubey}{2}
				\pgfmathsetmacro{\cubez}{2}
				\draw[blue,fill=pastelgreen] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (0,0,0) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
				\draw[blue,fill=pastelgreen] (0,0,0) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
				
				\draw[blue,fill=pastelgreen] (-\cubex-1,1,0) -- ++(-\cubex,0,0) -- ++(0,-\cubey-2,0) -- ++(\cubex,0,0) -- cycle;
				\draw[blue,fill=pastelgreen] (\cubex+2+1,0,-\cubey) -- ++(-\cubex-2,0,0) -- ++(0,-\cubey,0) -- ++(\cubex+2,0,0) -- cycle;
				
				\draw[blue,fill=pastelgreen] (0,0,-\cubez-1) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez-2) -- ++(\cubex,0,0) -- cycle;
				
				\path (-18,0) -- (8,0);
				\end{tikzpicture}}
			\vspace*{-0.25cm}\caption{CP and Tucker decompositions of a 3-dimensional tensor.\label{fig:CTdecompositions}}\vspace*{-0.5cm}
		\end{center}		
	\end{figure}
	
	Figure~\ref{fig:CTdecompositions} shows visual representations of both tensor decompositions for a 3-dimensional tensor. CP decomposition represents a tensor with the sum of rank one tensors. Whereas, Tucker decomposition represents a tensor with multiple factor matrices and a much smaller core tensor. As mentioned earlier, tensors are used to store data in several domains. For example, in neuroscience, a single trial to observe behaviors of neurons over time can be stored in a matrix and data of multiple trials in a 3-dimensional tensor. CP decomposition is a popular method to analyse such neuroscience data~\cite{WKV-Neuron-2018}.
%%	\bora{this was said before}. 
	Similarly, a single experiment in combustion simulations or electron microscopy produces terabytes of data. For instance, tracking 64 variables on a 512$\times$512$\times$512 three-dimensional spatial grid for 128 time steps in a simulation requires to store $2^{40}$ entries. It is very difficult to transfer and analyze data of such experiments. Tucker decomposition is an appealing practice to compress such data before different types of analysis can be performed~\cite{ABK-IPDPS-2016}.
	
	
	%%Computations involving Tucker-format tensors, such as tensor inner products, often require far fewer operations than with their full-format, dense representations. As a result, the Tucker decomposition is often used as a dimensionality reduction technique before other types of analysis are done.
	
	
%    Tensor Train is another popular tensor representation. This is very well suited to work with high dimensional tensors and used often in molecular and quantum simulations. However, this has limited use in data analytics as it is hard to interpret its different components. Therefore, we focus on CP and Tucker decompositions in this project and leave Tensor Train decomposition for future work. 
%    Therefore, we focus on CP and Tucker decompositions that are widely used for data analytics.

	
	
%%	Recent advances in computing architectures enable us to witness many high performance computing (HPC) systems which perform more than $1$ million billion operations per second -- \href{https://www.olcf.ornl.gov/frontier}{Frontier} at ORNL and \href{https://www.genci.fr/en/our-computers}{Adastra} at GENCI, for example. Such systems have a large number of computing units and enable us to tackle some of the science and societal issues which was not possible before. Traditionally, there has been a great emphasis on developing algorithms which minimize the number of computations. In the last few decades, the rate of computations in these systems has improved drastically, but we have noticed relatively smaller improvement for the rate of data movement. The current HPC systems face bottleneck due to the large volume of communications~\cite{DOE-Report-2014}. Thus it is extremely important to develop algorithms which take communication costs into account. This approach also reduces energy consumption of the computations. GPUs deliver increased processing capabilities and superior energy efficiency compared to CPUs. Therefore, they have become a crucial element of many HPC systems over the past decade. The SCATE project addresses communication costs and scalability of tensor decompositions on the modern HPC systems. More precisely, the high level aim of the project is to devise communication optimal tensor decomposition algorithms which scale well on both homogeneous and heterogeneous systems. Establishing communication lower bounds is also an important part of the project as it helps one to identify the more precise parallel algorithms that match the lower bounds.
	
	
	The data generated by many experiments is so big that it is impossible to perform a CP/Tucker decomposition without parallel computations. Therefore, it is important to focus on parallel algorithms for both decompositions.
%	Thanks to recent advances in architectures, we witness many parallel high performance computing (HPC) systems with a large number of computational units
%%	which perform more than $1$ million billion operations per second 
%	-- \href{https://www.olcf.ornl.gov/frontier}{Frontier} at ORNL and \href{https://www.genci.fr/en/our-computers}{Adastra} at GENCI, for example. 
	In the last few decades, the rate of computations has improved drastically, but we have noticed relatively smaller improvement in the rate of data movement. The current high performance computing (HPC) systems face bottleneck due to the large volume of communications~\cite{DOE-Report-2014}. Thus it is crucial to take communication costs into account while designing parallel algorithms. This approach also reduces energy consumption of the computations. GPUs deliver increased processing capabilities and superior energy efficiency compared to CPUs. Therefore, they have become a crucial element of many HPC systems over the past decade. The SCATE project addresses parallelization, communication costs and scalability of tensor decompositions on modern HPC systems. More precisely, the high level aim of the project is to devise parallel and communication optimal tensor decomposition algorithms which scale well on both homogeneous and heterogeneous systems. Establishing communication lower bounds is also an important part of the project as it helps one to devise efficient parallel algorithms.
%	identify the more precise parallel algorithms that match the lower bounds.	
	
	
%	Most existing tensor decomposition algorithms work with matrix (2-dimensional) representations of tensors at each step and rely on the parallelization of matrix operations. This approach neglects multi-dimensional properties of tensors and may not perform the computation efficiently. We focus on improving the performance of Tucker and CP decomposition algorithms by taking multi-dimensional properties of tensors into account.
	
	
	
	\vspace*{-0.325cm}\paragraph{Algorithms to compute Tucker decompositions:}
	Truncated higher-order SVD (HOSVD) is one of the popular algorithms for computing a Tucker decomposition, thanks to its quasi-optimal numerical approximation. In this algorithm, the core tensor is computed with the full-format original tensor and multiple tall and skinny factor matrices. For a 3-dimensional tensor $\X$, computation of the core tensor $\Y$ can be expressed in tensor notation as $\Y = \X \times_1 {\Mn{A}{1}}^\Tra \times_2 {\Mn{A}{2}}^\Tra \times_3 {\Mn{A}{3}}^\Tra$, where 
	%$\X$ is the full-format 
	%representation of the original 
	%tensor,
	$\Mn{A}{k}$ is a tall-skinny factor matrix corresponding to mode $k$, and $\times_k$ denotes the tensor-times-matrix (TTM) operation in the $k$th mode~\cite{KB-SIAM-2009}. This collective operation is known as the Multi-TTM computation~\cite{ABGKR-SIMAX-2022} and is one of the main bottlenecks of the HOSVD algorithm. One approach to perform this computation is \emph{in~sequence}, i.e., $\Y = ((\X \times_1 {\Mn{A}{1}}^\Tra )\times_2 {\Mn{A}{2}}^\Tra) \times_3 {\Mn{A}{3}}^\Tra$. Another approach is to work with the inputs \emph{all~at~once}, i.e., $\Y_{ijk} = \sum_{lmp}\X_{lmp}\cdot\Mn{A}{1}_{li}\cdot \Mn{A}{2}_{mj}\cdot \Mn{A}{3}_{pk}$.
%%	\bora{why * and $\times$} 
	Daas et al.~\cite{ABGKR-SIMAX-2022} show that the latter approach takes multi-dimensional properties of tensors into account and reduces communication significantly compared to the sequence approach for small and moderate number of processors. However there is not any clear winner for all settings. We can achieve better gains with a hybrid method that combines the strengths of both approaches. This perspective is not explored in~\cite{ABGKR-SIMAX-2022}.
%	We can obtain better gains by a hybrid method which combines the best of both approaches. 
	Furthermore, the amount of operations for the Multi-TTM computation depends on how factor matrices are operated with the input tensor. It is also important to model the computation-communication tradeoff in the combined hybrid method such that the overall completion time of the Multi-TTM is minimized.
	
%	In addition, HOSVD algorithm also computes SVD for each mode to obtain the corresponding factor matrix. Optimal data distributions to perform SVDs and the Multi-TTM computation may not be compatible and may require to perform extra communication. At last, our goal is to determine communication lower bound and communication-computation tradeoff for the complete HOSVD algorithm and devise a method which achieves the optimal completion time. As SVD is expensive, we also plan to look at other alternatives like randomized SVD to compute factor matrices.
	



%Considering optimal data distributions for only one approach may induce significant data movements for the other approach. Using optimal data distributions for both approaches may not be compatible or require significant data movement during the transition of the approach. 

	%%When the computational costs of the matrix SVDs are reduced using randomization, Multi-TTM becomes the overwhelming bottleneck computation [22, 25].
	
	%% Talk about MTTKRP
	
	\vspace*{-0.325cm}\paragraph{Algorithms to compute CP decompositions:}
	Computing a CP decomposition involves solving a nonlinear optimization problem to minimize the approximation error. The workhorse algorithm to compute this decomposition uses an alternating least squares approach. This works in multiple iterations. For a $d$-dimensional decomposition, in each iteration, $d$ matricized-tensor times Khatri-Rao product (MTTKRP) computations are performed. In this computation, a matrix representation of the original tensor is multiplied with the Khatri-Rao product of $d-1$ factor matrices. This is the bottleneck computation of the algorithm. 
	Ballard et al.~\cite{BNR-IPDPS-2018} show that the working
%%	\bora{did not understand}\sk{I added more details.}
	with all the inputs at once without forming intermediates reduces communication costs significantly compared to the procedure where the Khatri-Rao product is computed first and the intermediate output is multiplied with a matricized representation of the original tensor. Certain operations can be reused among all MTTKRPs of a single iteration. But, their approach restricts the reuse of operations as intermediates are not formed. The amount of operations in the MTTKRP computation also depends on how factor matrices are operated with the original tensor.
%%	different inputs are grouped in the procedure.
	It is important to study tradeoff among communications, computations and reuse of intermediate results for the MTTKRP computations and design a method which achieves optimal completion time.\vspace*{-0.125cm}
	
%	Similar to the previous plan, at last, our goal is to devise a method for a single iteration of the algorithm which achieves optimal completion time.
%	Another state-of-the-art approach to compute a CP decomposition is based on gradients. MTTKRP is also the bottleneck computation for this approach. Hence, improving the performance of MTTKRP will also accelerate gradient-based approach to compute the decomposition.
	
    \vspace*{-0.325cm}\paragraph{Challenges and our approaches:} As mentioned earlier, computation and communication costs of Multi-TTM and MTTKRP depend on how factor matrices are operated with the input tensor. If we do not select it carefully, their completion times may be very far from the optimal ones. Even when we know how we want to perform the computations, combining \emph{all~at~once} and \emph{in~sequence} approaches, using one after the other, in a straightforward way will only work if data distributions of inputs and temporaries between both approaches are compatible. However we cannot expect any guarantees with respect to the optimal communication/computation costs. Therefore, in order to take benefits of both \emph{all~at~once} and \emph{in~sequence} approaches, it is important to analyze all possible ways to combine them. We will study how to perform 3 and 4-dimensional Multi-TTM and MTTKRP computations in detail as the number of ways are limited. After that, we will generalize our findings and create a dynamic programming based approach to determine how to perform d-dimensional computations.
    
   	\vspace*{-0.325cm}\paragraph{Objectives:} The goal of the SCATE project is to devise Tucker and CP decomposition algorithms that scale well on modern computing systems. The project first studies tradeoff among computations, communications and data reuse for the existing algorithms and proposes new methods whose completion times are optimal for homogeneous systems. After that, the proposed methods will be implemented and tested with real-world data-sets from neuroscience and combustion simulations. This implementation will also be used to accelerate hyperspectral data analysis in astrophysics and brain-computer interface data analysis in electrophysiology. Significant amount of performance is produced by accelerators for the present HPC systems. So developing only homogeneous algorithms ignores too much computation power and may result in poor efficiency on heterogeneous systems. The project does address this concern and extends our framework for heterogeneous systems with GPUs.
% 	 CPUs and GPUs.
   	
%   	\vspace*{-0.6cm}\section*{Requested Budget}\vspace*{-0.375cm}
   	\vspace*{-0.325cm}\paragraph{Budget:} A budget of 12.5K euros would help to bootstrap the project, demonstrate its feasibility, obtain preliminary results to convince ANR (or similar agencies), and establish new collaborations. A proposal has also been submitted to the ANR JCJC call. The budget includes 4K euros to hire a six-month intern at M2 level who will conduct a detailed study of communication and computational costs for a 3-dimensional Multi-TTM with cubical tensors.
   	

%   	\vspace*{-0.325cm}\paragraph{Budget:} A budget of 25K euros would help to bootstrap the project, demonstrate its feasibility, obtain preliminary results to convince ANR (or similar agencies), and establish new collaborations. A proposal has also been submitted to the ANR JCJC call. The budget includes 8K euros to hire two six-month interns at M2 level who will work on the following topics: 1) Extend the framework to model communication costs on heterogeneous systems and examine the performance of a simple linear algebra computation, matrix multiplication, with the proposed extension, and 2) A detailed study of communication and computational costs for a 3-dimensional Multi-TTM with cubical tensors.
   	
   	
   	
   	
%   	\vspace*{-0.325cm}\paragraph{Budget:} A budget of 25K euros would enable the proposed research to be consolidated and make it easier to obtain an ANR JCJC funding. This budget includes 8K euros to hire two six-month interns at M2 level. Two topics can easily be extracted from the project: 1) Extend the framework to model communication costs on heterogeneous systems and examine the performance of a simple linear algebra computation, matrix multiplication, with the proposed extension, and 2) A detailed study of communication and computational costs for a 3-dimensional Multi-TTM with cubical tensors.
   	
%   	A study to utilize tensor cores of the current GPUS for the existing tensor decomposition algorithms.
   	
   	In addition to internship funding, the requested budget will enable us to participate in international conferences and workshops (SIAM CSE, SC, IPDPS, HiPC) with 1K euros for 2025 and 3K euros for the remaining two years, in order to meet the scientific-community and build new collaborations. It also includes partial support (4.5K euros) for the PI to attend the Complexity and Linear Algebra (\url{https://simons.berkeley.edu/programs/complexity-linear-algebra}) program for one-month at Simons Institute, Berkeley, California, USA. This will allow him to have fruitful discussion with matrix and tensor computation experts and start more collaborations.


	
	
	
	%%\vspace*{1ex}
	\vspace*{-0.215cm}
	{\footnotesize
		\bibliographystyle{IEEEtranS}
		\bibliography{scate}
	}
\vspace*{-0.215cm}
	

%	
%\sk{	
%    \begin{itemize}
%%        \item Why we do not consider tensor train decomposition
%        \item Mention applications from astrophysic and electrophysiology
%        \item Rewording of the last paragraph
%        \item What technical challenges associated with combining all-at-once and in-sequence approaches
%        \item Types of datasets
%        \item Real world applications
%    \end{itemize}
%}

%\emph{Abstract:}
%%
%Tensors are multi-dimensional arrays and used to store data in several domains, e.g., data mining, neuroscience, astrophysics, scientific simulations and computer vision. Tensor decompositions help to identify inherent structure of data, achieve data compression and enable various ways of data analysis. The computational and memory requirements of tensor operations grow exponentially with the number of dimensions. It is paramount to devise parallel tensor decomposition algorithms that make effective utilization of the modern computing systems. The principal challenges are data transfer costs of tensor operations and limited parallelism exposed by most existing algorithms. This project aims at solving these challenges and proposing new decomposition algorithms which scale well on current and future computing systems. Finally, the proposed algorithms will be implemented for large scale distributed systems and evaluated on real-world tensors from neuroscience and combustion simulations. The implementation will also be used to accelerate hyperspectral data analysis in astrophysics and brain-computer interface data analysis in electrophysiology.
%
%
%
%
%\emph{French Abstract:}
%%Les tenseurs sont des tableaux multidimensionnels utilisés pour stocker des données dans plusieurs domaines, par exemple pour la fouille de données, en neurosciences, en astrophysique, dans les simulations scientifiques et pour la vision par ordinateur. Les décompositions de tenseurs permettent d'identifier la structure inhérente des données, de les compresser et de les analyser de diverses manières. Les besoins en calcul et en mémoire des opérations tensorielles augmentent de manière exponentielle avec le nombre de dimensions. Il est primordial de concevoir des algorithmes parallèles de décomposition de tenseurs qui utilisent efficacement les systèmes informatiques modernes.  Les principaux défis sont les coûts importants de transfert de données et le parallélisme limité de la plupart des algorithmes existants.  Ce projet vise à résoudre ces défis et à proposer de nouveaux algorithmes de décomposition qui s'adaptent bien aux systèmes informatiques actuels et futurs. Enfin, les algorithmes proposés seront implantés sur des machines de calcul distribuées à grande échelle et évalués sur des tenseurs provenant de divers domaines applicatifs. La mise en oeuvre sera également utilisée pour accélérer l'analyse des données hyperspectrales en astrophysique et l'analyse des données d'interface cerveau-ordinateur en électrophysiologie.
%
%Les tenseurs sont des tableaux multidimensionnels utilisés pour stocker des données dans plusieurs domaines, par exemple l'exploration de données, les neurosciences, l'astrophysique, les simulations scientifiques et la vision par ordinateur. Les décompositions de tenseurs permettent d'identifier la structure intrinsèque des données, de les compresser et de les analyser de diverses manières. Les besoins en calcul et en mémoire des opérations tensorielles augmentent de manière exponentielle avec le nombre de dimensions. Il est primordial de concevoir des algorithmes parallèles de décomposition de tenseurs qui utilisent efficacement les systèmes informatiques modernes. Les principaux défis sont les coûts de transfert de données des opérations tensorielles et le parallélisme limité exposé par la plupart des algorithmes existants. Ce projet vise à résoudre ces défis et à proposer de nouveaux algorithmes de décomposition qui s'adaptent bien aux systèmes informatiques actuels et futurs. Enfin, les algorithmes proposés seront implémentés et testés sur des systèmes distribués à grande échelle et évalués sur des tenseurs réels issus de la neuroscience et de simulations de combustion. L'implémentation sera également utilisée pour accélérer l'analyse des données hyperspectrales en astrophysique et l'analyse des données d'interface cerveau-machine en électrophysiologie.



%Les tenseurs sont des tableaux multidimensionnels utilisés pour stocker des données dans plusieurs domaines, par exemple l'exploration de données, les neurosciences, l'astrophysique, les simulations scientifiques et la vision par ordinateur. Les décompositions de tenseurs permettent d'identifier la structure inhérente des données, de les compresser et de les analyser de diverses manières. Les besoins en calcul et en mémoire des opérations tensorielles augmentent de manière exponentielle avec le nombre de dimensions. Il est primordial de concevoir des algorithmes parallèles de décomposition de tenseurs qui utilisent efficacement les systèmes informatiques modernes. Les principaux défis sont les coûts de transfert de données des opérations tensorielles et le parallélisme limité exposé par la plupart des algorithmes existants. Ce projet vise à résoudre ces défis et à proposer de nouveaux algorithmes de décomposition qui s'adaptent bien aux systèmes informatiques actuels et futurs. Enfin, les algorithmes proposés seront mis en œuvre pour des systèmes distribués à grande échelle et évalués sur des tenseurs réels issus de la neuroscience et de simulations de combustion. La mise en œuvre sera également utilisée pour accélérer l'analyse des données hyperspectrales en astrophysique et l'analyse des données d'interface cerveau-ordinateur en électrophysiologie.



\end{document}

